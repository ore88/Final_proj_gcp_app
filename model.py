# -*- coding: utf-8 -*-
"""msds434_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oHCjVYEG0opTj68kqZo188_V8f2LYHoh
"""

import os
import pandas as pd
from pprint import pprint
from google.cloud import storage
from pycaret.regression import *

os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r'/content/drive/MyDrive/gcp_key.json'

storage_client = storage.Client()

bucket_name = 'msdsproject_finlproj'

# create a new bucket
bucket = storage_client.bucket(bucket_name)
#bucket.storage_class = 'COLDLINE' # Archive | Nearline | Standard
bucket.location = 'US' # Taiwan
bucket = storage_client.create_bucket(bucket) # returns Bucket object

"""
Upload File
"""
def upload_to_bucket(blob_name, file_path, bucket_name):
    bucket = storage_client.get_bucket(bucket_name)
    blob = bucket.blob(blob_name)
    blob.upload_from_filename(file_path)
    return blob
file_path = r'/content/drive/MyDrive/msds_project_data'
upload_to_bucket('test', os.path.join(file_path,'test.csv'), 'msdsproject_finlproj')
upload_to_bucket('train', os.path.join(file_path,'train.csv'), 'msdsproject_finlproj')

"""
Download File By Passing URI Path
"""
def download_file_from_bucket(blob_name, file_path, bucket_name):
    bucket = storage_client.get_bucket(bucket_name)
    blob = bucket.blob(blob_name)
    with open(file_path, 'wb') as f:
        storage_client.download_blob_to_file(blob, f)
    print('Saved')

bucket_name = 'msdsproject_finlproj'
download_file_from_bucket('test', os.path.join(os.getcwd(), 'test.csv'), bucket_name)
download_file_from_bucket('train', os.path.join(os.getcwd(), 'train.csv'), bucket_name)

if __name__ == '__main__':
    bucket_name = 'msdsproject_finlproj'
    source_blob_name = 'test'
    destination_file_name = '/content/drive/MyDrive/msds_project_data/datatouse/test.csv'
    #DOWNLOAD
    storage_client = storage.Client()
    bucket = storage_client.get_bucket(bucket_name)
    blob = bucket.blob(source_blob_name)
    blob.download_to_filename(destination_file_name)
    print('Blob {} downloaded to {}.'.format(source_blob_name, destination_file_name))

if __name__ == '__main__':
    bucket_name = 'msdsproject_finlproj'
    source_blob_name = 'train'
    destination_file_name = '/content/drive/MyDrive/msds_project_data/datatouse/train.csv'
    #DOWNLOAD
    storage_client = storage.Client()
    bucket = storage_client.get_bucket(bucket_name)
    blob = bucket.blob(source_blob_name)
    blob.download_to_filename(destination_file_name)
    print('Blob {} downloaded to {}.'.format(source_blob_name, destination_file_name))


df_train = pd.read_csv('/content/drive/MyDrive/msds_project_data/datatouse/train.csv')
df_test = pd.read_csv('/content/drive/MyDrive/msds_project_data/datatouse/test.csv')

exp_reg101 = setup(data = df_train, target = 'price_range', session_id=123)

compare_models(fold=5)

model = create_model('knn', fold=5, round=2)
tuned_knn = tune_model(model)

# Checking score after cross-validation:
predict_model(model);

unseen_predictions = predict_model(tuned_knn, data=df_test)

unseen_predictions

model_dir= '/content/drive/MyDrive/msds_project_data/'
model_name = 'tuned_knn_gcp'
final_knn = finalize_model(tuned_knn)
# Saving model to google drive

save_model(final_knn, model_dir + model_name)

saved_final_knn = load_model(model_dir + model_name)

new_prediction = predict_model(saved_final_knn, data=df_test)
new_prediction.head()

# GCP project name, Change the name based on your own GCP project.
CLOUD_PROJECT = 'grand-drive-342813' # GCP project name
bucket_name = 'msdsproject_finlproj' # bucket name for storage of your model
BUCKET = 'gs://' + CLOUD_PROJECT + '-{}'.format(bucket_name)
# Set the gcloud consol to $CLOUD_PROJECT Environment Variable for your Desired Project)

def create_bucket(project_name, bucket_name):
    """Creates a new bucket."""
    # bucket_name = "your-new-bucket-name"

    storage_client = storage.Client(project_name)

    buckets = storage_client.list_buckets()

    if bucket_name not in buckets:
      bucket = storage_client.create_bucket(bucket_name)
      print("Bucket {} created".format(bucket.name))
    else:
      raise FileExistsError('{} already exists'.format(bucket_name))


def upload_blob(project_name, bucket_name, source_file_name, destination_blob_name):
    """Uploads a file to the bucket."""
    # bucket_name = "your-bucket-name"
    # source_file_name = "local/path/to/file"
    # destination_blob_name = "storage-object-name"

    storage_client = storage.Client(project_name)
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(destination_blob_name)

    blob.upload_from_filename(source_file_name)

    print(
        "File {} uploaded to {}.".format(
            source_file_name, destination_blob_name
        )
    )

def download_blob(project_name, bucket_name, source_blob_name, destination_file_name):
    """Downloads a blob from the bucket."""
    # bucket_name = "your-bucket-name"
    # source_blob_name = "storage-object-name"
    # destination_file_name = "local/path/to/file"

    storage_client = storage.Client(project_name)

    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(source_blob_name)
    
    if destination_file_name is not None: 
      blob.download_to_filename(destination_file_name)

      print(
          "Blob {} downloaded to {}.".format(
              source_blob_name, destination_file_name
          )
      )
    
  
    return blob

# Create Bucket
# create_bucket(CLOUD_PROJECT, bucket_name)

# Save Model Local/google drive and upload to GCP
model_name_gcp = 'knn-reg101-gcp'
save_model(final_knn, model_name= model_dir + model_name_gcp, verbose=False)
model_src = model_dir + model_name_gcp +'.pkl'
model_dst = str(model_name)+'.pkl'
upload_blob(CLOUD_PROJECT, bucket_name, model_src, model_dst)

outfile_name = model_dir + 'knn-reg101-gcp-downloaded'
model_gcp_src = str(model_name)+'.pkl'
model_downloaded = download_blob(CLOUD_PROJECT, bucket_name, model_gcp_src, outfile_name + '.pkl')

os.listdir(model_dir)

# Loading the model for predictions
gcp_final_knn = load_model(outfile_name)

# Predictions from deployed model
new_prediction_gcp = predict_model(gcp_final_knn, data=df_test)
print("Done")
#https://github.com/pycaret/pycaret/blob/master/examples/PyCaret2-Regression-Predicting_ENEM_Math_Grades.ipynb
#https://towardsdatascience.com/build-pycaret-deploy-gcp-521415a6c330